{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "warming-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chinese-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sorted-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('data/wine.csv')\n",
    "\n",
    "y = wine.iloc[:, -2]   # y = wine.iloc[:, 11]\n",
    "del wine['quality'] # 정답컬럼 삭제\n",
    "X = wine\n",
    "\n",
    "y_en = pd.get_dummies(y) # 정답컬럼을 7개로 원핫인코딩\n",
    "\n",
    "# 7:3 비율로 train,test 나눔\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_en, test_size = 0.3,\n",
    "#                                                    random_state = 1)\n",
    "\n",
    "# 출력층의 활성화함수는 softmax로 설정해주자.\n",
    "# compile loss = 'categorical_crossentropy'\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim = 12, activation = 'relu')) # 입력층\n",
    "model.add(Dense(48, activation = 'relu'))  # 은닉층1\n",
    "model.add(Dense(24, activation = 'relu'))  # 은닉층2\n",
    "model.add(Dense(7, activation = 'softmax')) # 출력층\n",
    "\n",
    "# softmax 다중분류시 쓰는 활성화함수\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-february",
   "metadata": {},
   "source": [
    "### 1 모델 실행 및 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mediterranean-mistress",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surgical-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broke-hometown",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.32259, saving model to ./model2/01-1.3226.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.32259 to 1.30002, saving model to ./model2/02-1.3000.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.30002\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.30002 to 1.29319, saving model to ./model2/08-1.2932.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.29319 to 1.27718, saving model to ./model2/09-1.2772.hdf5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.27718\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.27718\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.27718\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.27718 to 1.26709, saving model to ./model2/13-1.2671.hdf5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.26709\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.26709 to 1.23772, saving model to ./model2/19-1.2377.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.23772 to 1.23349, saving model to ./model2/20-1.2335.hdf5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.23349\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.23349\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.23349 to 1.23087, saving model to ./model2/23-1.2309.hdf5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.23087\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.23087\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.23087 to 1.21585, saving model to ./model2/26-1.2158.hdf5\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.21585 to 1.19197, saving model to ./model2/27-1.1920.hdf5\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.19197 to 1.17191, saving model to ./model2/28-1.1719.hdf5\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.17191 to 1.15233, saving model to ./model2/29-1.1523.hdf5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.15233\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.15233\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.15233 to 1.14045, saving model to ./model2/32-1.1404.hdf5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.14045\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.14045 to 1.13472, saving model to ./model2/34-1.1347.hdf5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.13472\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.13472\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.13472\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.13472\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.13472\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.13472 to 1.12448, saving model to ./model2/40-1.1245.hdf5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.12448\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.12448\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.12448\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.12448 to 1.12355, saving model to ./model2/44-1.1236.hdf5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.12355\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.12355\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.12355 to 1.10955, saving model to ./model2/47-1.1096.hdf5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.10955\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.10955 to 1.10441, saving model to ./model2/54-1.1044.hdf5\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.10441 to 1.09414, saving model to ./model2/55-1.0941.hdf5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.09414\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.09414\n",
      "\n",
      "Epoch 00058: val_loss improved from 1.09414 to 1.09081, saving model to ./model2/58-1.0908.hdf5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.09081\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.09081\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.09081\n",
      "\n",
      "Epoch 00062: val_loss improved from 1.09081 to 1.09009, saving model to ./model2/62-1.0901.hdf5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.09009\n",
      "\n",
      "Epoch 00075: val_loss improved from 1.09009 to 1.08876, saving model to ./model2/75-1.0888.hdf5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.08876\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.08876\n",
      "\n",
      "Epoch 00078: val_loss improved from 1.08876 to 1.06870, saving model to ./model2/78-1.0687.hdf5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.06870\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.06870\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.06870\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.06870\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.06870\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.06870\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.06870\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.06870\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.06870\n",
      "\n",
      "Epoch 00088: val_loss improved from 1.06870 to 1.06483, saving model to ./model2/88-1.0648.hdf5\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.06483\n",
      "\n",
      "Epoch 00106: val_loss improved from 1.06483 to 1.06411, saving model to ./model2/106-1.0641.hdf5\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.06411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00138: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.06411\n",
      "\n",
      "Epoch 00213: val_loss improved from 1.06411 to 1.05580, saving model to ./model2/213-1.0558.hdf5\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 1.05580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00296: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 1.05580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00455: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 1.05580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00614: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 1.05580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00773: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 1.05580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00931: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 1.05580\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 1.05580\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = './model2'\n",
    "\n",
    "# 폴더가 없다면 폴더 생성\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    # 저장할 파일명의 정규식 설정 {epoch의 수 - 손시값을 파일명으로 설정}\n",
    "\n",
    "modelpath = MODEL_DIR + \"/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor = 'val_loss', save_best_only = True, verbose = 1)\n",
    "\n",
    "history = model.fit(X, y_en, validation_split = 0.33, epochs = 1000, batch_size = 100, verbose = 0, callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "retained-dylan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_model = tf.keras.models.load_model('model2/11-1.3376.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mediterranean-projection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497/6497 [==============================] - 0s 43us/sample - loss: 1.0340 - accuracy: 0.5809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.033993932766714, 0.5808835]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_model.evaluate(X, y_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vanilla-joint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3225925919059274,\n",
       " 1.300017669484332,\n",
       " 1.3253165077774118,\n",
       " 1.3056190739144813,\n",
       " 1.305861580066192,\n",
       " 1.3121490764729071,\n",
       " 1.3021830289791791,\n",
       " 1.2931923471686446,\n",
       " 1.2771825470846572,\n",
       " 1.3065473669336671,\n",
       " 1.2781791987119022,\n",
       " 1.2975734163831163,\n",
       " 1.2670855913962518,\n",
       " 1.341800011677064,\n",
       " 1.2780009302503856,\n",
       " 1.279100368906568,\n",
       " 1.2773583632804852,\n",
       " 1.2677280474931767,\n",
       " 1.2377194327352208,\n",
       " 1.2334925122472233,\n",
       " 1.2726606967010143,\n",
       " 1.2895796854734976,\n",
       " 1.2308650850416063,\n",
       " 1.2660082473065748,\n",
       " 1.2877442706159223,\n",
       " 1.215845235299953,\n",
       " 1.1919740582957412,\n",
       " 1.1719112949215726,\n",
       " 1.152327468762031,\n",
       " 1.1775829497472945,\n",
       " 1.1888497456526146,\n",
       " 1.140447193926031,\n",
       " 1.1676227460096489,\n",
       " 1.1347210786559365,\n",
       " 1.196875718090084,\n",
       " 1.1521731356640796,\n",
       " 1.1391750796413644,\n",
       " 1.1350989601550958,\n",
       " 1.14435774949325,\n",
       " 1.1244760968468406,\n",
       " 1.1437916538932107,\n",
       " 1.2229407100410727,\n",
       " 1.1246044193948066,\n",
       " 1.1235534066642636,\n",
       " 1.1548302398694978,\n",
       " 1.1667538689566659,\n",
       " 1.109554452495975,\n",
       " 1.234681042202147,\n",
       " 1.3087290939473328,\n",
       " 1.1866293776007522,\n",
       " 1.117494089481158,\n",
       " 1.129501962300503,\n",
       " 1.2332139312526285,\n",
       " 1.1044146770086043,\n",
       " 1.0941410275883885,\n",
       " 1.2784730109857234,\n",
       " 1.1924113201928306,\n",
       " 1.0908080678441863,\n",
       " 1.148005274903802,\n",
       " 1.1924608251153728,\n",
       " 1.2186227089319475,\n",
       " 1.0900873150025214,\n",
       " 1.2383254538604986,\n",
       " 1.1262101726932126,\n",
       " 1.2324669341663104,\n",
       " 1.0991742826286173,\n",
       " 1.1165090087966207,\n",
       " 1.145216498897348,\n",
       " 1.2138743575636324,\n",
       " 1.1298568400191815,\n",
       " 1.0976524258549119,\n",
       " 1.1429119493577864,\n",
       " 1.107675603914372,\n",
       " 1.0922655003332036,\n",
       " 1.0887645249600177,\n",
       " 1.121119027371173,\n",
       " 1.1247257258230712,\n",
       " 1.068704818234299,\n",
       " 1.1276892016539763,\n",
       " 1.1307639012803565,\n",
       " 1.1189548138416174,\n",
       " 1.0969500409695374,\n",
       " 1.1203364169958867,\n",
       " 1.1341759813971175,\n",
       " 1.1174312932785853,\n",
       " 1.1291676567984628,\n",
       " 1.1667428475159864,\n",
       " 1.064831542385208,\n",
       " 1.257768082451987,\n",
       " 1.2081170340518017,\n",
       " 1.1272299842678861,\n",
       " 1.0977343466732052,\n",
       " 1.2300995449364047,\n",
       " 1.2250349293777716,\n",
       " 1.0732222086899763,\n",
       " 1.1363595685203036,\n",
       " 1.2111077714355398,\n",
       " 1.1210944958222218,\n",
       " 1.1484757620137889,\n",
       " 1.1749690054179904,\n",
       " 1.1872392284286606,\n",
       " 1.0693092615732225,\n",
       " 1.173648715019226,\n",
       " 1.1507451034092402,\n",
       " 1.0863951987875646,\n",
       " 1.0641116994522113,\n",
       " 1.131771692029246,\n",
       " 1.0863637528219423,\n",
       " 1.124832087185555,\n",
       " 1.1151533771506001,\n",
       " 1.2180163943684184,\n",
       " 1.0861707904955724,\n",
       " 1.0805936118105908,\n",
       " 1.115145678564663,\n",
       " 1.104348101954916,\n",
       " 1.1016665784629074,\n",
       " 1.2453037633762494,\n",
       " 1.0877354249253974,\n",
       " 1.1842820360943989,\n",
       " 1.158005168665817,\n",
       " 1.1647988060415486,\n",
       " 1.2339115481832366,\n",
       " 1.0711943518309603,\n",
       " 1.164644996325175,\n",
       " 1.1823742459148239,\n",
       " 1.0825532959613489,\n",
       " 1.0765830432618415,\n",
       " 1.0802190263510305,\n",
       " 1.0852815685294448,\n",
       " 1.128630611724231,\n",
       " 1.174065958369862,\n",
       " 1.170531301231651,\n",
       " 1.1073423090514602,\n",
       " 1.0821359604944438,\n",
       " 1.0725677898435881,\n",
       " 1.1134459545840194,\n",
       " 1.1650232129441553,\n",
       " 1.141707896630525,\n",
       " 1.0705863419390502,\n",
       " 1.1179750321350452,\n",
       " 1.1100629615339088,\n",
       " 1.1260516546267174,\n",
       " 1.1370659813736423,\n",
       " 1.101238016759877,\n",
       " 1.1203903326621423,\n",
       " 1.0946613976727555,\n",
       " 1.1229118072625361,\n",
       " 1.1055872504249875,\n",
       " 1.1882965378550105,\n",
       " 1.087182116397333,\n",
       " 1.0669282041229569,\n",
       " 1.077395685347088,\n",
       " 1.095373342642973,\n",
       " 1.1100655295354225,\n",
       " 1.1609298082498403,\n",
       " 1.0834894202528023,\n",
       " 1.1098807820907006,\n",
       " 1.1650186779893639,\n",
       " 1.151780735918414,\n",
       " 1.187785763006944,\n",
       " 1.1592144096370065,\n",
       " 1.1859296460251707,\n",
       " 1.0951356803064858,\n",
       " 1.1058131363286283,\n",
       " 1.1201611279329775,\n",
       " 1.133345932671518,\n",
       " 1.1758969524801473,\n",
       " 1.2239112220444046,\n",
       " 1.0853629699953786,\n",
       " 1.154293320951484,\n",
       " 1.1168259368354068,\n",
       " 1.1193651850129063,\n",
       " 1.0800288337649722,\n",
       " 1.1277101103798215,\n",
       " 1.1162594494286118,\n",
       " 1.1500394455480687,\n",
       " 1.0829355472173445,\n",
       " 1.1241829298037194,\n",
       " 1.179049967607974,\n",
       " 1.1335608272841482,\n",
       " 1.1863410878570486,\n",
       " 1.0833184959449413,\n",
       " 1.149713972509602,\n",
       " 1.1071545506690765,\n",
       " 1.19799487879782,\n",
       " 1.114301310135768,\n",
       " 1.141555291789395,\n",
       " 1.0651964935667309,\n",
       " 1.13361840259223,\n",
       " 1.080792670761233,\n",
       " 1.1736759594826154,\n",
       " 1.0751892618643932,\n",
       " 1.1261061945439497,\n",
       " 1.1134042602318983,\n",
       " 1.1129977008401652,\n",
       " 1.1537284023000365,\n",
       " 1.094173366512174,\n",
       " 1.2588576623054095,\n",
       " 1.0832595889107053,\n",
       " 1.1678086899933005,\n",
       " 1.1391870872679846,\n",
       " 1.165967855698023,\n",
       " 1.097560601356702,\n",
       " 1.0732386595719343,\n",
       " 1.0992523863876895,\n",
       " 1.0818217178602596,\n",
       " 1.0852142400119131,\n",
       " 1.0663027974553319,\n",
       " 1.0904963180457516,\n",
       " 1.0821143961175061,\n",
       " 1.1107242622853437,\n",
       " 1.0870922973383834,\n",
       " 1.0558029428506508,\n",
       " 1.1007728691979166,\n",
       " 1.159890348261053,\n",
       " 1.087466005678777,\n",
       " 1.0974973795853016,\n",
       " 1.1679071396380871,\n",
       " 1.1128920026314564,\n",
       " 1.1382589009536175,\n",
       " 1.0792230091172776,\n",
       " 1.1492451237631844,\n",
       " 1.1506136261221969,\n",
       " 1.170154938331017,\n",
       " 1.0804195980647784,\n",
       " 1.1237399912936425,\n",
       " 1.1202371591454618,\n",
       " 1.1233732708684214,\n",
       " 1.1136577540622168,\n",
       " 1.1594158533291938,\n",
       " 1.1402247316075926,\n",
       " 1.1028780166085783,\n",
       " 1.1799525806120226,\n",
       " 1.1453959052657192,\n",
       " 1.1776581816462093,\n",
       " 1.0968024775142715,\n",
       " 1.116924218364529,\n",
       " 1.1068511620545998,\n",
       " 1.2673204562047145,\n",
       " 1.1084630387924213,\n",
       " 1.0858547611114306,\n",
       " 1.198405452263661,\n",
       " 1.1210744986723076,\n",
       " 1.1084293767566726,\n",
       " 1.0946760524958719,\n",
       " 1.1399541229079098,\n",
       " 1.1001180400659432,\n",
       " 1.116756061990778,\n",
       " 1.1130755513142316,\n",
       " 1.1065270656194441,\n",
       " 1.1310969392458599,\n",
       " 1.0921398197020684,\n",
       " 1.1417942816838795,\n",
       " 1.1339237188403701,\n",
       " 1.1520661896481104,\n",
       " 1.0979386326316354,\n",
       " 1.1893963233018532,\n",
       " 1.1317709599619423,\n",
       " 1.1833513575000363,\n",
       " 1.0989453856483762,\n",
       " 1.0837160582309002,\n",
       " 1.0967222789784412,\n",
       " 1.1194918426600369,\n",
       " 1.1023171471826958,\n",
       " 1.133367929425273,\n",
       " 1.106788082278414,\n",
       " 1.2289421060980061,\n",
       " 1.1945515212479172,\n",
       " 1.0929065909419027,\n",
       " 1.1098415734884621,\n",
       " 1.2195747232103682,\n",
       " 1.1633295854488452,\n",
       " 1.1185058091348146,\n",
       " 1.073968645853874,\n",
       " 1.1216306521064474,\n",
       " 1.1487171263683649,\n",
       " 1.0883306127328138,\n",
       " 1.1151589967987754,\n",
       " 1.0930389153373825,\n",
       " 1.1001636781892576,\n",
       " 1.125775648719503,\n",
       " 1.0693785120279362,\n",
       " 1.142266553876561,\n",
       " 1.1745967589891875,\n",
       " 1.1451685673151262,\n",
       " 1.129902118708426,\n",
       " 1.1409365400012002,\n",
       " 1.1050054790812494,\n",
       " 1.1064370488509154,\n",
       " 1.176515449177135,\n",
       " 1.1531622698812773,\n",
       " 1.2378867761794226,\n",
       " 1.1146297996694392,\n",
       " 1.0987596349282698,\n",
       " 1.103048279429927,\n",
       " 1.1312059565579697,\n",
       " 1.1044973298942014,\n",
       " 1.098735667192019,\n",
       " 1.1540137482134056,\n",
       " 1.248206395925064,\n",
       " 1.1036101842260027,\n",
       " 1.1917020168615666,\n",
       " 1.1754130633560929,\n",
       " 1.1463085560254005,\n",
       " 1.158262271425385,\n",
       " 1.0718322693606912,\n",
       " 1.1756986254578703,\n",
       " 1.065437895295781,\n",
       " 1.113556314042676,\n",
       " 1.0792254612440273,\n",
       " 1.23732018970943,\n",
       " 1.1692676822066586,\n",
       " 1.1581052897415516,\n",
       " 1.1556130248905618,\n",
       " 1.1340631850949534,\n",
       " 1.1684988837142092,\n",
       " 1.1337981304644427,\n",
       " 1.1309161326546213,\n",
       " 1.2145817871693965,\n",
       " 1.1348453304428598,\n",
       " 1.1329119743167104,\n",
       " 1.1444311432349377,\n",
       " 1.1362654019346883,\n",
       " 1.1475473094931294,\n",
       " 1.1694139300526438,\n",
       " 1.0837367646621936,\n",
       " 1.203209389895548,\n",
       " 1.1945112618533047,\n",
       " 1.1032431892581753,\n",
       " 1.1421020977146976,\n",
       " 1.1278899060540544,\n",
       " 1.120281440950496,\n",
       " 1.1540013587836064,\n",
       " 1.148450147160839,\n",
       " 1.1304049671113074,\n",
       " 1.1776528486282953,\n",
       " 1.164490992094809,\n",
       " 1.1920294589373892,\n",
       " 1.200375833433547,\n",
       " 1.1064893596377963,\n",
       " 1.2341435817016033,\n",
       " 1.174068467322485,\n",
       " 1.1558199799977815,\n",
       " 1.112438514932886,\n",
       " 1.2135323383313514,\n",
       " 1.242130662177826,\n",
       " 1.1720164716938437,\n",
       " 1.1121900345617797,\n",
       " 1.1410805327353222,\n",
       " 1.1035480318647441,\n",
       " 1.1562302154260915,\n",
       " 1.125044589415019,\n",
       " 1.161632014182342,\n",
       " 1.1601527448578592,\n",
       " 1.1646191687572809,\n",
       " 1.1450288209603938,\n",
       " 1.1115166843632163,\n",
       " 1.121248172574388,\n",
       " 1.125983408419958,\n",
       " 1.1536953315868244,\n",
       " 1.1684471379904757,\n",
       " 1.1369197396409538,\n",
       " 1.1191592534661015,\n",
       " 1.0972417872268836,\n",
       " 1.1558740626681934,\n",
       " 1.10071368998303,\n",
       " 1.1396417027308947,\n",
       " 1.209497597945598,\n",
       " 1.139336071370087,\n",
       " 1.1374598983284476,\n",
       " 1.1385998534155892,\n",
       " 1.1803336240750648,\n",
       " 1.1604123315611086,\n",
       " 1.1734772809457668,\n",
       " 1.1949997959714946,\n",
       " 1.093916809225416,\n",
       " 1.2149121875251645,\n",
       " 1.1725320999438946,\n",
       " 1.1468633498067344,\n",
       " 1.1576388908988668,\n",
       " 1.154083639591724,\n",
       " 1.1221131874131156,\n",
       " 1.1707338094711304,\n",
       " 1.1426272722946735,\n",
       " 1.1539606075464706,\n",
       " 1.1668700491076027,\n",
       " 1.225397637674025,\n",
       " 1.2284665966367387,\n",
       " 1.101885732098337,\n",
       " 1.2301244124388084,\n",
       " 1.1492330494460525,\n",
       " 1.2117744709228302,\n",
       " 1.1684651149736418,\n",
       " 1.138077049683302,\n",
       " 1.1988213120640574,\n",
       " 1.1479017364951003,\n",
       " 1.115553479511421,\n",
       " 1.1173195298457201,\n",
       " 1.1793963672953607,\n",
       " 1.1732507282759481,\n",
       " 1.2141852145428425,\n",
       " 1.1363754353045306,\n",
       " 1.123155634025316,\n",
       " 1.176405836930086,\n",
       " 1.1339268107792158,\n",
       " 1.181451688557516,\n",
       " 1.1420277391836082,\n",
       " 1.1392966639467608,\n",
       " 1.196354686797082,\n",
       " 1.1260638841382273,\n",
       " 1.2530389332271124,\n",
       " 1.1238553612938016,\n",
       " 1.133190087505154,\n",
       " 1.2362630036605267,\n",
       " 1.120232259199058,\n",
       " 1.1458221664239754,\n",
       " 1.1169157589510048,\n",
       " 1.170749257355581,\n",
       " 1.1270681387616759,\n",
       " 1.1972796666872252,\n",
       " 1.1166526905862324,\n",
       " 1.1702397003040448,\n",
       " 1.1168311510608468,\n",
       " 1.112332740585843,\n",
       " 1.1710508700573083,\n",
       " 1.146200544906385,\n",
       " 1.2400715626203096,\n",
       " 1.1289656944486088,\n",
       " 1.126723478724073,\n",
       " 1.1360261247152492,\n",
       " 1.2741967417421318,\n",
       " 1.127739587863842,\n",
       " 1.1778706751225434,\n",
       " 1.141377632295613,\n",
       " 1.2311793701909917,\n",
       " 1.1183350637798264,\n",
       " 1.2436043565923518,\n",
       " 1.1715411584138316,\n",
       " 1.186505977090422,\n",
       " 1.213023389136041,\n",
       " 1.1546257959379183,\n",
       " 1.1640171054359916,\n",
       " 1.1352193673729618,\n",
       " 1.155553708404372,\n",
       " 1.098126616372373,\n",
       " 1.1294753509801585,\n",
       " 1.1451697396787452,\n",
       " 1.1650444381720537,\n",
       " 1.1476529886672546,\n",
       " 1.21955870831763,\n",
       " 1.1694310375582644,\n",
       " 1.2127056802625145,\n",
       " 1.2436133047917506,\n",
       " 1.1533872975613966,\n",
       " 1.202809874550168,\n",
       " 1.232027125803185,\n",
       " 1.1463930401212963,\n",
       " 1.1449323528574342,\n",
       " 1.1851830635315332,\n",
       " 1.2411097610469186,\n",
       " 1.1743209164737265,\n",
       " 1.114287723889162,\n",
       " 1.1417568666395885,\n",
       " 1.185426438049281,\n",
       " 1.1862193754502943,\n",
       " 1.2137423167417656,\n",
       " 1.1303102454661211,\n",
       " 1.1784046392340761,\n",
       " 1.1650891456848536,\n",
       " 1.2026682208468031,\n",
       " 1.1980070676003303,\n",
       " 1.2510179033924094,\n",
       " 1.1667640020241548,\n",
       " 1.2021029127505554,\n",
       " 1.1189555028379659,\n",
       " 1.1384814678927957,\n",
       " 1.184731066643775,\n",
       " 1.1864318573947275,\n",
       " 1.2530720119987613,\n",
       " 1.1680031757810454,\n",
       " 1.1681385101416173,\n",
       " 1.1781174409917463,\n",
       " 1.1703683921507189,\n",
       " 1.2760702427172717,\n",
       " 1.215190974148837,\n",
       " 1.1926021706529986,\n",
       " 1.2185508732473378,\n",
       " 1.176129236226871,\n",
       " 1.2851728487125922,\n",
       " 1.2018222699076424,\n",
       " 1.2259865729680983,\n",
       " 1.1769032728421938,\n",
       " 1.3127788880488256,\n",
       " 1.1946201855208212,\n",
       " 1.183923337704096,\n",
       " 1.1701722453524184,\n",
       " 1.2004525216865094,\n",
       " 1.1803226721036684,\n",
       " 1.1913861422549872,\n",
       " 1.1732649894861074,\n",
       " 1.1817663084099066,\n",
       " 1.1894231942983775,\n",
       " 1.2265186079176433,\n",
       " 1.1734067578137894,\n",
       " 1.1699222991794418,\n",
       " 1.207356430711724,\n",
       " 1.2508768262840928,\n",
       " 1.1653060165596454,\n",
       " 1.2081638674635988,\n",
       " 1.2179989775855502,\n",
       " 1.1472977649915468,\n",
       " 1.1765212370799139,\n",
       " 1.1878026063903506,\n",
       " 1.1474442554242683,\n",
       " 1.2008809772404758,\n",
       " 1.2141826758573662,\n",
       " 1.169467390139342,\n",
       " 1.1964518590406938,\n",
       " 1.1499033703670636,\n",
       " 1.2242953716180263,\n",
       " 1.1848322314260167,\n",
       " 1.1504450665487276,\n",
       " 1.2623888273617048,\n",
       " 1.2170845150669694,\n",
       " 1.3042146148103657,\n",
       " 1.212058673649679,\n",
       " 1.2485820977560012,\n",
       " 1.192172760452146,\n",
       " 1.169429685408141,\n",
       " 1.2578326938869235,\n",
       " 1.2402357955078978,\n",
       " 1.171970182643348,\n",
       " 1.2054623079188775,\n",
       " 1.1809300753897998,\n",
       " 1.187346305602636,\n",
       " 1.1924123159655324,\n",
       " 1.216931207474573,\n",
       " 1.245535025507698,\n",
       " 1.1940110577292098,\n",
       " 1.2201222548118005,\n",
       " 1.2085840442241766,\n",
       " 1.1436083289015266,\n",
       " 1.1564952183714559,\n",
       " 1.1874395328246075,\n",
       " 1.2209238569775382,\n",
       " 1.2115337865335958,\n",
       " 1.184884509423396,\n",
       " 1.2131143519372651,\n",
       " 1.2268870471518634,\n",
       " 1.2077650957174235,\n",
       " 1.2015265862702769,\n",
       " 1.1776698038572475,\n",
       " 1.1654569075935648,\n",
       " 1.2156640908101222,\n",
       " 1.226631107030215,\n",
       " 1.2000838586222597,\n",
       " 1.178946849488434,\n",
       " 1.2191022477505646,\n",
       " 1.2613517407215002,\n",
       " 1.1777226810966617,\n",
       " 1.3172356354328858,\n",
       " 1.2001710252050475,\n",
       " 1.1847574212056495,\n",
       " 1.1879426101982455,\n",
       " 1.1687398187486164,\n",
       " 1.2011749074731395,\n",
       " 1.1390511047590028,\n",
       " 1.167124483690951,\n",
       " 1.2307984634434983,\n",
       " 1.2105567874886216,\n",
       " 1.24459049807284,\n",
       " 1.3122677786366923,\n",
       " 1.2216441250625467,\n",
       " 1.2296947540936771,\n",
       " 1.226895937136003,\n",
       " 1.2254610475682435,\n",
       " 1.2477577521806553,\n",
       " 1.2186916922077988,\n",
       " 1.221243712174031,\n",
       " 1.2403988315786794,\n",
       " 1.1770934877973613,\n",
       " 1.1963234214515952,\n",
       " 1.2011827355498201,\n",
       " 1.1956519630405453,\n",
       " 1.2246689073967212,\n",
       " 1.2178303993943131,\n",
       " 1.2472985393795377,\n",
       " 1.185627219282386,\n",
       " 1.3015129983008324,\n",
       " 1.232303839185577,\n",
       " 1.3661669892864627,\n",
       " 1.228485758487995,\n",
       " 1.2585150545293635,\n",
       " 1.3170978489455643,\n",
       " 1.1753550298286206,\n",
       " 1.2451723908488845,\n",
       " 1.274572306023889,\n",
       " 1.2160965740541756,\n",
       " 1.333364695380062,\n",
       " 1.2219637143306243,\n",
       " 1.2009124598858796,\n",
       " 1.1749843714676258,\n",
       " 1.295345242206867,\n",
       " 1.2135400308039916,\n",
       " 1.2126962785676365,\n",
       " 1.218156126551417,\n",
       " 1.212708748303927,\n",
       " 1.2273361932981264,\n",
       " 1.2160075738991336,\n",
       " 1.2451091023869725,\n",
       " 1.1929159850785227,\n",
       " 1.1791374491644906,\n",
       " 1.275995175321619,\n",
       " 1.198492076569226,\n",
       " 1.2521631871903693,\n",
       " 1.2046898825741037,\n",
       " 1.2566770098426125,\n",
       " 1.2499239922681333,\n",
       " 1.2907433826606591,\n",
       " 1.2269064907705312,\n",
       " 1.263534993836374,\n",
       " 1.304303221491389,\n",
       " 1.2822740083529955,\n",
       " 1.2673956508680935,\n",
       " 1.2202119142303378,\n",
       " 1.2483936945597331,\n",
       " 1.1990520520643755,\n",
       " 1.2285318592767338,\n",
       " 1.2990859321780972,\n",
       " 1.2967966622683829,\n",
       " 1.306558271665951,\n",
       " 1.2454165467571268,\n",
       " 1.3284138713405405,\n",
       " 1.2853105393323032,\n",
       " 1.18526432230756,\n",
       " 1.2605937267794753,\n",
       " 1.263550821162048,\n",
       " 1.3085687168828257,\n",
       " 1.2095775802930195,\n",
       " 1.2223881866548445,\n",
       " 1.3011967618982274,\n",
       " 1.289094014601274,\n",
       " 1.2790084340911367,\n",
       " 1.2877470518881347,\n",
       " 1.195929871294604,\n",
       " 1.253386682702667,\n",
       " 1.192499352640761,\n",
       " 1.2401428449126113,\n",
       " 1.215493615968522,\n",
       " 1.2785657324713149,\n",
       " 1.2226573009313126,\n",
       " 1.2914443355062346,\n",
       " 1.2281726976652523,\n",
       " 1.2712672573703152,\n",
       " 1.308113852303067,\n",
       " 1.2088937004963,\n",
       " 1.217912709518468,\n",
       " 1.2298068122152404,\n",
       " 1.257825463385015,\n",
       " 1.3059865297415318,\n",
       " 1.2319120441005502,\n",
       " 1.2989125571328721,\n",
       " 1.210979979493957,\n",
       " 1.2655394777551399,\n",
       " 1.314747335193874,\n",
       " 1.191955919727023,\n",
       " 1.2292520990182747,\n",
       " 1.2632432461896421,\n",
       " 1.2503441398516124,\n",
       " 1.2265243620583506,\n",
       " 1.2179670126843842,\n",
       " 1.2423852017987302,\n",
       " 1.2924796651293349,\n",
       " 1.3448108920962105,\n",
       " 1.1983685568496063,\n",
       " 1.332454790046443,\n",
       " 1.2659188639589678,\n",
       " 1.2180763663667622,\n",
       " 1.2375803949671746,\n",
       " 1.3218843069943516,\n",
       " 1.2045235036414264,\n",
       " 1.2567117603230866,\n",
       " 1.2199130748813247,\n",
       " 1.274469313087997,\n",
       " 1.2686464680936231,\n",
       " 1.3378174177138678,\n",
       " 1.2143374192409027,\n",
       " 1.288940180987467,\n",
       " 1.1918854707882398,\n",
       " 1.2637863264772997,\n",
       " 1.2495475440592199,\n",
       " 1.2234966454806027,\n",
       " 1.2707688730635565,\n",
       " 1.2534858299579932,\n",
       " 1.2824991833079944,\n",
       " 1.232908315175063,\n",
       " 1.2518727751600716,\n",
       " 1.2526632597396423,\n",
       " 1.2614748169492174,\n",
       " 1.220232732646115,\n",
       " 1.3204378331457818,\n",
       " 1.2506951088116163,\n",
       " 1.281307445261584,\n",
       " 1.3284234517104143,\n",
       " 1.3099837945057795,\n",
       " 1.2601907539478827,\n",
       " 1.2349376096314206,\n",
       " 1.2751684044346665,\n",
       " 1.311193412556237,\n",
       " 1.255124691205147,\n",
       " 1.254748920182804,\n",
       " 1.2421126713008035,\n",
       " 1.2969571971671008,\n",
       " 1.2366175929427425,\n",
       " 1.2771339908346429,\n",
       " 1.3472878102100256,\n",
       " 1.2739641316287167,\n",
       " 1.278777585857676,\n",
       " 1.270705527775771,\n",
       " 1.3165078974548197,\n",
       " 1.3346745417668269,\n",
       " 1.2413152846700939,\n",
       " 1.305253619358534,\n",
       " 1.2649403932767036,\n",
       " 1.2267847839213195,\n",
       " 1.3455000443336291,\n",
       " 1.3385175963937541,\n",
       " 1.263133432620611,\n",
       " 1.3554541531142654,\n",
       " 1.2604212441366591,\n",
       " 1.2476755928048442,\n",
       " 1.2762446698053178,\n",
       " 1.2946403115501492,\n",
       " 1.328509717276602,\n",
       " 1.264958026386919,\n",
       " 1.287203750688157,\n",
       " 1.272918696031148,\n",
       " 1.2809397025263949,\n",
       " 1.341818612772268,\n",
       " 1.2645109272225477,\n",
       " 1.22637936063024,\n",
       " 1.3161531372781679,\n",
       " 1.3202057132076273,\n",
       " 1.248990698016329,\n",
       " 1.3730359252516207,\n",
       " 1.278566624179031,\n",
       " 1.238473211412941,\n",
       " 1.3893892328778068,\n",
       " 1.4070951274502805,\n",
       " 1.2945235120110856,\n",
       " 1.341724562200355,\n",
       " 1.2258443757370636,\n",
       " 1.3058951940291967,\n",
       " 1.3594068947371902,\n",
       " 1.3129022738316676,\n",
       " 1.277027235164509,\n",
       " 1.3038198522754483,\n",
       " 1.2842172839980581,\n",
       " 1.2355018588768574,\n",
       " 1.2597367830209798,\n",
       " 1.3192686890110825,\n",
       " 1.339705228527665,\n",
       " 1.2802340413306976,\n",
       " 1.3368251432072034,\n",
       " 1.2970396305297638,\n",
       " 1.2367050643567439,\n",
       " 1.41262200773457,\n",
       " 1.2738096311097935,\n",
       " 1.3069829054367847,\n",
       " 1.2746965276611435,\n",
       " 1.2603949344519414,\n",
       " 1.280527373849651,\n",
       " 1.3920563462175133,\n",
       " 1.2475798916427683,\n",
       " 1.252894869217506,\n",
       " 1.3479057928343197,\n",
       " 1.28553196127876,\n",
       " 1.3520404581145529,\n",
       " 1.3055320711402627,\n",
       " 1.2795637657036592,\n",
       " 1.2549764748775598,\n",
       " 1.2742162395468404,\n",
       " 1.2619955532756442,\n",
       " 1.2730281251571673,\n",
       " 1.4356945472441631,\n",
       " 1.2687503430115314,\n",
       " 1.2841343946390218,\n",
       " 1.2958370641672805,\n",
       " 1.3014815726202407,\n",
       " 1.2865743492588853,\n",
       " 1.306948231094645,\n",
       " 1.3193182108841297,\n",
       " 1.2861654219371734,\n",
       " 1.2245257617710354,\n",
       " 1.3021133515662524,\n",
       " 1.3713810819012302,\n",
       " 1.2991645905799243,\n",
       " 1.2656785271940254,\n",
       " 1.3419805659558668,\n",
       " 1.2963562359065164,\n",
       " 1.3679892555539146,\n",
       " 1.3155792707051985,\n",
       " 1.301643240424025,\n",
       " 1.3449849007012962,\n",
       " 1.283792071687036,\n",
       " 1.2658152552473518,\n",
       " 1.3732617234850264,\n",
       " 1.3197619459290049,\n",
       " 1.2933172740580596,\n",
       " 1.3274823110420386,\n",
       " 1.3501848146354123,\n",
       " 1.2736795304260609,\n",
       " 1.296284316303013,\n",
       " 1.2724382646711834,\n",
       " 1.3309326680390152,\n",
       " 1.3482885157867468,\n",
       " 1.3259874532272766,\n",
       " 1.348100102864779,\n",
       " 1.2833492063975835,\n",
       " 1.3185525301730994,\n",
       " 1.3327937065026698,\n",
       " 1.3152963317635453,\n",
       " 1.3525000868977366,\n",
       " 1.3140674611349483,\n",
       " 1.3435826062600373,\n",
       " 1.3038364294525626,\n",
       " 1.2630171474321183,\n",
       " 1.2949246947581952,\n",
       " 1.4209328020091379,\n",
       " 1.313316494295925,\n",
       " 1.3853763742046756,\n",
       " 1.3157290307791916,\n",
       " 1.297330453123524,\n",
       " 1.349770038833707,\n",
       " 1.3558037936826408,\n",
       " 1.2612990432685904,\n",
       " 1.2710433370305663,\n",
       " 1.3438873063156378,\n",
       " 1.275393395157127,\n",
       " 1.3691284867711278,\n",
       " 1.3388203162969132,\n",
       " 1.3298681929950669,\n",
       " 1.3771768247053062,\n",
       " 1.2581201249902898,\n",
       " 1.4584903847643267,\n",
       " 1.3403766210818346,\n",
       " 1.358208348145296,\n",
       " 1.4819211456881258,\n",
       " 1.307178943863004,\n",
       " 1.309877992509962,\n",
       " 1.3815119038650763,\n",
       " 1.3447577219742994,\n",
       " 1.3135672841038737,\n",
       " 1.3065079692360404,\n",
       " 1.324173617474127,\n",
       " 1.3656770293807095,\n",
       " 1.3021231206146988,\n",
       " 1.2800431687792977,\n",
       " 1.3115883295908397,\n",
       " 1.2868680723341472,\n",
       " 1.397987786706511,\n",
       " 1.324413049193251,\n",
       " 1.302678310787761,\n",
       " 1.3046030174324286,\n",
       " 1.3015524535190253,\n",
       " 1.3708361084366734,\n",
       " 1.4129438294675245,\n",
       " 1.3257854723985933,\n",
       " 1.288932111435559,\n",
       " 1.3752302801136649,\n",
       " 1.3407634816247544,\n",
       " 1.408251950512955,\n",
       " 1.328865400561086,\n",
       " 1.3615534964141311,\n",
       " 1.31848273771904,\n",
       " 1.4266605560596173,\n",
       " 1.3573314954628755,\n",
       " 1.3584103550944295,\n",
       " 1.333166180512844,\n",
       " 1.3476138484505784,\n",
       " 1.3665055014314629,\n",
       " 1.4275980115214706,\n",
       " 1.4730130770267584,\n",
       " 1.4102244468835683,\n",
       " 1.4050165937219188,\n",
       " 1.3127345986021703,\n",
       " 1.356757254867287,\n",
       " 1.3734156433796827,\n",
       " 1.2853506121324214,\n",
       " 1.381887667662614,\n",
       " 1.3501415747306842,\n",
       " 1.3063173997096527,\n",
       " 1.349454240365462,\n",
       " 1.348962813824207,\n",
       " 1.2905232089382785,\n",
       " 1.3218386059318668,\n",
       " 1.3746444531531878,\n",
       " 1.3229919157939636,\n",
       " 1.3701842926043175,\n",
       " 1.3808639255159108,\n",
       " 1.3681466810075276,\n",
       " 1.2965469916105825,\n",
       " 1.3718154349805036,\n",
       " 1.393329881565832,\n",
       " 1.415142742903916,\n",
       " 1.3548242123254808,\n",
       " 1.313010556714518,\n",
       " 1.2890066323858318,\n",
       " 1.355419781102445,\n",
       " 1.3291702059321193,\n",
       " 1.3120000866743236,\n",
       " 1.2991811294099946,\n",
       " 1.2812130238626387,\n",
       " 1.3932866349920525,\n",
       " 1.3537002888037053,\n",
       " 1.3411157317372746,\n",
       " 1.4006605923592628,\n",
       " 1.4279211856824257,\n",
       " 1.3342062032028235,\n",
       " 1.3224510104506166,\n",
       " 1.3489131657949416,\n",
       " 1.3135118738754645,\n",
       " 1.3592510695779796,\n",
       " 1.3666739797258711,\n",
       " 1.4237784466821275,\n",
       " 1.331465290698694,\n",
       " 1.4278884298040038,\n",
       " 1.3100397803566672,\n",
       " 1.4493367913719657,\n",
       " 1.4481139383115968,\n",
       " 1.3476693016110044,\n",
       " 1.2975742950306073,\n",
       " 1.3538100197042897,\n",
       " 1.3749596311217978,\n",
       " 1.3989074869311495,\n",
       " 1.3986617482347643,\n",
       " 1.3467314293334534,\n",
       " 1.369464865653387,\n",
       " 1.3594918904048858,\n",
       " 1.501357358374518,\n",
       " 1.4041533900863363,\n",
       " 1.4454858803248907,\n",
       " 1.3715879289142459,\n",
       " 1.324271495664592,\n",
       " 1.4168051161688247,\n",
       " 1.396099010825435,\n",
       " 1.3412450852093998,\n",
       " 1.4037916065651777,\n",
       " 1.3358287066568584,\n",
       " 1.4691914372788721,\n",
       " 1.3257461436144955,\n",
       " 1.3432786028424064,\n",
       " 1.3978526728414433,\n",
       " 1.372134499060802,\n",
       " 1.333315886539735,\n",
       " 1.3663820777183924,\n",
       " 1.404753098121056,\n",
       " 1.3447436019257233,\n",
       " 1.3404141838178212,\n",
       " 1.357251099217466,\n",
       " 1.3618213427372468,\n",
       " 1.4781244501367317,\n",
       " 1.4318204076140078,\n",
       " 1.3251413490388777,\n",
       " 1.4532527378944806,\n",
       " 1.3434835633475741,\n",
       " 1.3925983558445822,\n",
       " 1.4194438840959456,\n",
       " 1.4092011857421805,\n",
       " 1.349802794712129,\n",
       " 1.418787332125755,\n",
       " 1.4332936026833274,\n",
       " 1.4023398730582568,\n",
       " 1.406061527612326,\n",
       " 1.3963151461594587,\n",
       " 1.4043735821486074,\n",
       " 1.4167043918218367,\n",
       " 1.472861080736547,\n",
       " 1.3795683125515918,\n",
       " 1.3952306970293984,\n",
       " 1.383673872981038,\n",
       " 1.4208454624478355,\n",
       " 1.3745453596670867,\n",
       " 1.3558225628935097,\n",
       " 1.4726026864040704,\n",
       " 1.4105187815663975,\n",
       " 1.387830733836114,\n",
       " 1.3798793704359682,\n",
       " 1.3765834028070623,\n",
       " 1.3919734993736783,\n",
       " 1.3580464394220384,\n",
       " 1.4057343725835805,\n",
       " 1.5118851678354757,\n",
       " 1.353005184160246,\n",
       " 1.3990500481812271,\n",
       " 1.404298139618827,\n",
       " 1.3467484123501188,\n",
       " 1.3216035561961728,\n",
       " 1.37463598345821,\n",
       " 1.4240357225591487]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "overhead-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "therapeutic-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_acc = history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "gentle-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = np.arange(len(y_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-clark",
   "metadata": {},
   "source": [
    "### 그래프로 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "solved-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "independent-petite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_len, y_loss, c = 'red', markersize = 3)\n",
    "plt.plot(x_len, y_acc, c = \"blue\", markersize = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-breathing",
   "metadata": {},
   "source": [
    "### 자동 학습 중단 (EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "commercial-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "lesser-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "light-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5197 samples, validate on 1300 samples\n",
      "Epoch 1/2000\n",
      "5197/5197 [==============================] - 0s 10us/sample - loss: 0.9748 - accuracy: 0.6034 - val_loss: 1.2539 - val_accuracy: 0.4831\n",
      "Epoch 2/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9479 - accuracy: 0.6048 - val_loss: 1.2448 - val_accuracy: 0.4908\n",
      "Epoch 3/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9403 - accuracy: 0.6048 - val_loss: 1.2889 - val_accuracy: 0.4585\n",
      "Epoch 4/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9330 - accuracy: 0.6061 - val_loss: 1.2365 - val_accuracy: 0.4738\n",
      "Epoch 5/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9330 - accuracy: 0.6113 - val_loss: 1.2507 - val_accuracy: 0.4677\n",
      "Epoch 6/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9298 - accuracy: 0.6088 - val_loss: 1.2525 - val_accuracy: 0.4577\n",
      "Epoch 7/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9260 - accuracy: 0.6111 - val_loss: 1.2308 - val_accuracy: 0.4731\n",
      "Epoch 8/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9264 - accuracy: 0.6075 - val_loss: 1.2239 - val_accuracy: 0.4792\n",
      "Epoch 9/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9264 - accuracy: 0.6090 - val_loss: 1.2199 - val_accuracy: 0.4785\n",
      "Epoch 10/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9246 - accuracy: 0.6121 - val_loss: 1.2444 - val_accuracy: 0.4700\n",
      "Epoch 11/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9237 - accuracy: 0.6075 - val_loss: 1.2427 - val_accuracy: 0.4638\n",
      "Epoch 12/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9243 - accuracy: 0.6061 - val_loss: 1.2314 - val_accuracy: 0.4823\n",
      "Epoch 13/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9237 - accuracy: 0.6052 - val_loss: 1.2146 - val_accuracy: 0.4992\n",
      "Epoch 14/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9250 - accuracy: 0.6105 - val_loss: 1.2286 - val_accuracy: 0.4915\n",
      "Epoch 15/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9262 - accuracy: 0.6052 - val_loss: 1.2130 - val_accuracy: 0.4969\n",
      "Epoch 16/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9206 - accuracy: 0.6080 - val_loss: 1.2131 - val_accuracy: 0.4908\n",
      "Epoch 17/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9212 - accuracy: 0.6111 - val_loss: 1.2170 - val_accuracy: 0.4923\n",
      "Epoch 18/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9198 - accuracy: 0.6142 - val_loss: 1.2205 - val_accuracy: 0.4785\n",
      "Epoch 19/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9181 - accuracy: 0.6107 - val_loss: 1.2232 - val_accuracy: 0.4746\n",
      "Epoch 20/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9187 - accuracy: 0.6073 - val_loss: 1.2434 - val_accuracy: 0.4692\n",
      "Epoch 21/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9198 - accuracy: 0.6169 - val_loss: 1.2830 - val_accuracy: 0.4438\n",
      "Epoch 22/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9223 - accuracy: 0.6084 - val_loss: 1.2726 - val_accuracy: 0.4523\n",
      "Epoch 23/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9267 - accuracy: 0.6048 - val_loss: 1.2428 - val_accuracy: 0.4638\n",
      "Epoch 24/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9177 - accuracy: 0.6111 - val_loss: 1.1922 - val_accuracy: 0.5123\n",
      "Epoch 25/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9180 - accuracy: 0.6113 - val_loss: 1.2168 - val_accuracy: 0.4823\n",
      "Epoch 26/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9185 - accuracy: 0.6088 - val_loss: 1.2050 - val_accuracy: 0.4846\n",
      "Epoch 27/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9166 - accuracy: 0.6117 - val_loss: 1.2202 - val_accuracy: 0.4715\n",
      "Epoch 28/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9156 - accuracy: 0.6144 - val_loss: 1.2278 - val_accuracy: 0.4754\n",
      "Epoch 29/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9174 - accuracy: 0.6094 - val_loss: 1.1989 - val_accuracy: 0.4846\n",
      "Epoch 30/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9147 - accuracy: 0.6154 - val_loss: 1.2078 - val_accuracy: 0.4815\n",
      "Epoch 31/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9138 - accuracy: 0.6105 - val_loss: 1.2434 - val_accuracy: 0.4631\n",
      "Epoch 32/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9162 - accuracy: 0.6113 - val_loss: 1.2556 - val_accuracy: 0.4569\n",
      "Epoch 33/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9157 - accuracy: 0.6075 - val_loss: 1.2424 - val_accuracy: 0.4638\n",
      "Epoch 34/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9150 - accuracy: 0.6100 - val_loss: 1.2356 - val_accuracy: 0.4692\n",
      "Epoch 35/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9151 - accuracy: 0.6075 - val_loss: 1.2228 - val_accuracy: 0.4677\n",
      "Epoch 36/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9128 - accuracy: 0.6159 - val_loss: 1.2104 - val_accuracy: 0.4785\n",
      "Epoch 37/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9119 - accuracy: 0.6136 - val_loss: 1.2322 - val_accuracy: 0.4646\n",
      "Epoch 38/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9138 - accuracy: 0.6082 - val_loss: 1.2163 - val_accuracy: 0.4792\n",
      "Epoch 39/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9123 - accuracy: 0.6119 - val_loss: 1.2455 - val_accuracy: 0.4485\n",
      "Epoch 40/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9124 - accuracy: 0.6150 - val_loss: 1.2313 - val_accuracy: 0.4692\n",
      "Epoch 41/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9111 - accuracy: 0.6102 - val_loss: 1.2248 - val_accuracy: 0.4669\n",
      "Epoch 42/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9105 - accuracy: 0.6117 - val_loss: 1.2385 - val_accuracy: 0.4646\n",
      "Epoch 43/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9181 - accuracy: 0.6109 - val_loss: 1.2311 - val_accuracy: 0.4638\n",
      "Epoch 44/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9157 - accuracy: 0.6115 - val_loss: 1.2127 - val_accuracy: 0.4723\n",
      "Epoch 45/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9132 - accuracy: 0.6075 - val_loss: 1.2185 - val_accuracy: 0.4685\n",
      "Epoch 46/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9109 - accuracy: 0.6132 - val_loss: 1.2232 - val_accuracy: 0.4654\n",
      "Epoch 47/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9132 - accuracy: 0.6121 - val_loss: 1.2211 - val_accuracy: 0.4692\n",
      "Epoch 48/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9089 - accuracy: 0.6107 - val_loss: 1.2106 - val_accuracy: 0.4877\n",
      "Epoch 49/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9116 - accuracy: 0.6148 - val_loss: 1.2048 - val_accuracy: 0.4692\n",
      "Epoch 50/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9114 - accuracy: 0.6163 - val_loss: 1.1960 - val_accuracy: 0.4877\n",
      "Epoch 51/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9115 - accuracy: 0.6132 - val_loss: 1.2058 - val_accuracy: 0.4723\n",
      "Epoch 52/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9160 - accuracy: 0.6053 - val_loss: 1.2148 - val_accuracy: 0.4792\n",
      "Epoch 53/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9112 - accuracy: 0.6127 - val_loss: 1.1797 - val_accuracy: 0.5092\n",
      "Epoch 54/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9113 - accuracy: 0.6061 - val_loss: 1.1946 - val_accuracy: 0.4846\n",
      "Epoch 55/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9101 - accuracy: 0.6109 - val_loss: 1.2636 - val_accuracy: 0.4454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9127 - accuracy: 0.6140 - val_loss: 1.2239 - val_accuracy: 0.4700\n",
      "Epoch 57/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9112 - accuracy: 0.6138 - val_loss: 1.2039 - val_accuracy: 0.4769\n",
      "Epoch 58/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9101 - accuracy: 0.6113 - val_loss: 1.1941 - val_accuracy: 0.4792\n",
      "Epoch 59/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9074 - accuracy: 0.6134 - val_loss: 1.1911 - val_accuracy: 0.4877\n",
      "Epoch 60/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9073 - accuracy: 0.6125 - val_loss: 1.2155 - val_accuracy: 0.4731\n",
      "Epoch 61/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9099 - accuracy: 0.6098 - val_loss: 1.2057 - val_accuracy: 0.4869\n",
      "Epoch 62/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9115 - accuracy: 0.6096 - val_loss: 1.2217 - val_accuracy: 0.4723\n",
      "Epoch 63/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9092 - accuracy: 0.6125 - val_loss: 1.2055 - val_accuracy: 0.4808\n",
      "Epoch 64/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9092 - accuracy: 0.6134 - val_loss: 1.1889 - val_accuracy: 0.4892\n",
      "Epoch 65/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9079 - accuracy: 0.6105 - val_loss: 1.1956 - val_accuracy: 0.4923\n",
      "Epoch 66/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9108 - accuracy: 0.6136 - val_loss: 1.1757 - val_accuracy: 0.5238\n",
      "Epoch 67/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9114 - accuracy: 0.6107 - val_loss: 1.1746 - val_accuracy: 0.5085\n",
      "Epoch 68/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9120 - accuracy: 0.6138 - val_loss: 1.1860 - val_accuracy: 0.5192\n",
      "Epoch 69/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9107 - accuracy: 0.6115 - val_loss: 1.1889 - val_accuracy: 0.5008\n",
      "Epoch 70/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9129 - accuracy: 0.6115 - val_loss: 1.2038 - val_accuracy: 0.4862\n",
      "Epoch 71/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9079 - accuracy: 0.6148 - val_loss: 1.1723 - val_accuracy: 0.4985\n",
      "Epoch 72/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9108 - accuracy: 0.6084 - val_loss: 1.2055 - val_accuracy: 0.4885\n",
      "Epoch 73/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9113 - accuracy: 0.6121 - val_loss: 1.2354 - val_accuracy: 0.4669\n",
      "Epoch 74/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9079 - accuracy: 0.6138 - val_loss: 1.2272 - val_accuracy: 0.4738\n",
      "Epoch 75/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9060 - accuracy: 0.6119 - val_loss: 1.2126 - val_accuracy: 0.4846\n",
      "Epoch 76/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9081 - accuracy: 0.6129 - val_loss: 1.1976 - val_accuracy: 0.4800\n",
      "Epoch 77/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9070 - accuracy: 0.6140 - val_loss: 1.2148 - val_accuracy: 0.4669\n",
      "Epoch 78/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9072 - accuracy: 0.6134 - val_loss: 1.2040 - val_accuracy: 0.4831\n",
      "Epoch 79/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9079 - accuracy: 0.6123 - val_loss: 1.1969 - val_accuracy: 0.4838\n",
      "Epoch 80/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9141 - accuracy: 0.6032 - val_loss: 1.2146 - val_accuracy: 0.4785\n",
      "Epoch 81/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9079 - accuracy: 0.6138 - val_loss: 1.2212 - val_accuracy: 0.4669\n",
      "Epoch 82/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9067 - accuracy: 0.6127 - val_loss: 1.2171 - val_accuracy: 0.4769\n",
      "Epoch 83/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9057 - accuracy: 0.6138 - val_loss: 1.2052 - val_accuracy: 0.4800\n",
      "Epoch 84/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9052 - accuracy: 0.6105 - val_loss: 1.2025 - val_accuracy: 0.4792\n",
      "Epoch 85/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9053 - accuracy: 0.6119 - val_loss: 1.1796 - val_accuracy: 0.5123\n",
      "Epoch 86/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9090 - accuracy: 0.6171 - val_loss: 1.1721 - val_accuracy: 0.5108\n",
      "Epoch 87/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9137 - accuracy: 0.6113 - val_loss: 1.1534 - val_accuracy: 0.5369\n",
      "Epoch 88/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9147 - accuracy: 0.6067 - val_loss: 1.1790 - val_accuracy: 0.5346\n",
      "Epoch 89/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9161 - accuracy: 0.6071 - val_loss: 1.1749 - val_accuracy: 0.5285\n",
      "Epoch 90/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9128 - accuracy: 0.6075 - val_loss: 1.1891 - val_accuracy: 0.4931\n",
      "Epoch 91/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9083 - accuracy: 0.6088 - val_loss: 1.1954 - val_accuracy: 0.5023\n",
      "Epoch 92/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9058 - accuracy: 0.6130 - val_loss: 1.2140 - val_accuracy: 0.4754\n",
      "Epoch 93/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9043 - accuracy: 0.6130 - val_loss: 1.2041 - val_accuracy: 0.4885\n",
      "Epoch 94/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9053 - accuracy: 0.6125 - val_loss: 1.1913 - val_accuracy: 0.4923\n",
      "Epoch 95/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9053 - accuracy: 0.6165 - val_loss: 1.1836 - val_accuracy: 0.5162\n",
      "Epoch 96/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9089 - accuracy: 0.6059 - val_loss: 1.2087 - val_accuracy: 0.4846\n",
      "Epoch 97/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9075 - accuracy: 0.6115 - val_loss: 1.1719 - val_accuracy: 0.5092\n",
      "Epoch 98/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9095 - accuracy: 0.6144 - val_loss: 1.1847 - val_accuracy: 0.5038\n",
      "Epoch 99/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9040 - accuracy: 0.6146 - val_loss: 1.2056 - val_accuracy: 0.4869\n",
      "Epoch 100/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9033 - accuracy: 0.6136 - val_loss: 1.2418 - val_accuracy: 0.4677\n",
      "Epoch 101/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9039 - accuracy: 0.6163 - val_loss: 1.2249 - val_accuracy: 0.4738\n",
      "Epoch 102/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9040 - accuracy: 0.6123 - val_loss: 1.1945 - val_accuracy: 0.4938\n",
      "Epoch 103/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9081 - accuracy: 0.6125 - val_loss: 1.2044 - val_accuracy: 0.4977\n",
      "Epoch 104/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9049 - accuracy: 0.6140 - val_loss: 1.2071 - val_accuracy: 0.4769\n",
      "Epoch 105/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9040 - accuracy: 0.6140 - val_loss: 1.2374 - val_accuracy: 0.4746\n",
      "Epoch 106/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9064 - accuracy: 0.6129 - val_loss: 1.2248 - val_accuracy: 0.4731\n",
      "Epoch 107/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9032 - accuracy: 0.6155 - val_loss: 1.1967 - val_accuracy: 0.4885\n",
      "Epoch 108/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9031 - accuracy: 0.6134 - val_loss: 1.1902 - val_accuracy: 0.5092\n",
      "Epoch 109/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9044 - accuracy: 0.6136 - val_loss: 1.2116 - val_accuracy: 0.4838\n",
      "Epoch 110/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9073 - accuracy: 0.6127 - val_loss: 1.2234 - val_accuracy: 0.4808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9102 - accuracy: 0.6090 - val_loss: 1.2217 - val_accuracy: 0.4831\n",
      "Epoch 112/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9040 - accuracy: 0.6144 - val_loss: 1.2311 - val_accuracy: 0.4754\n",
      "Epoch 113/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9030 - accuracy: 0.6109 - val_loss: 1.2223 - val_accuracy: 0.4777\n",
      "Epoch 114/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9015 - accuracy: 0.6144 - val_loss: 1.2179 - val_accuracy: 0.4877\n",
      "Epoch 115/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9028 - accuracy: 0.6175 - val_loss: 1.1984 - val_accuracy: 0.4915\n",
      "Epoch 116/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9018 - accuracy: 0.6146 - val_loss: 1.2423 - val_accuracy: 0.4700\n",
      "Epoch 117/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9034 - accuracy: 0.6182 - val_loss: 1.2290 - val_accuracy: 0.4754\n",
      "Epoch 118/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9053 - accuracy: 0.6129 - val_loss: 1.1945 - val_accuracy: 0.4938\n",
      "Epoch 119/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9016 - accuracy: 0.6167 - val_loss: 1.2182 - val_accuracy: 0.4846\n",
      "Epoch 120/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9050 - accuracy: 0.6163 - val_loss: 1.2260 - val_accuracy: 0.4800\n",
      "Epoch 121/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9048 - accuracy: 0.6123 - val_loss: 1.2035 - val_accuracy: 0.5031\n",
      "Epoch 122/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9052 - accuracy: 0.6169 - val_loss: 1.2159 - val_accuracy: 0.4815\n",
      "Epoch 123/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9038 - accuracy: 0.6117 - val_loss: 1.1971 - val_accuracy: 0.4931\n",
      "Epoch 124/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9043 - accuracy: 0.6165 - val_loss: 1.2081 - val_accuracy: 0.4892\n",
      "Epoch 125/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9044 - accuracy: 0.6129 - val_loss: 1.2356 - val_accuracy: 0.4685\n",
      "Epoch 126/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9025 - accuracy: 0.6132 - val_loss: 1.2296 - val_accuracy: 0.4808\n",
      "Epoch 127/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9027 - accuracy: 0.6107 - val_loss: 1.2448 - val_accuracy: 0.4600\n",
      "Epoch 128/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9043 - accuracy: 0.6125 - val_loss: 1.2175 - val_accuracy: 0.4892\n",
      "Epoch 129/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9045 - accuracy: 0.6127 - val_loss: 1.2023 - val_accuracy: 0.4854\n",
      "Epoch 130/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9004 - accuracy: 0.6161 - val_loss: 1.2285 - val_accuracy: 0.4823\n",
      "Epoch 131/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9053 - accuracy: 0.6159 - val_loss: 1.1882 - val_accuracy: 0.5115\n",
      "Epoch 132/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9035 - accuracy: 0.6121 - val_loss: 1.2073 - val_accuracy: 0.4946\n",
      "Epoch 133/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9034 - accuracy: 0.6138 - val_loss: 1.2332 - val_accuracy: 0.4715\n",
      "Epoch 134/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9047 - accuracy: 0.6084 - val_loss: 1.2229 - val_accuracy: 0.4823\n",
      "Epoch 135/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9008 - accuracy: 0.6173 - val_loss: 1.2355 - val_accuracy: 0.4777\n",
      "Epoch 136/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8984 - accuracy: 0.6159 - val_loss: 1.2288 - val_accuracy: 0.4823\n",
      "Epoch 137/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9005 - accuracy: 0.6155 - val_loss: 1.2182 - val_accuracy: 0.4800\n",
      "Epoch 138/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8990 - accuracy: 0.6169 - val_loss: 1.2297 - val_accuracy: 0.4846\n",
      "Epoch 139/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8996 - accuracy: 0.6138 - val_loss: 1.2067 - val_accuracy: 0.4862\n",
      "Epoch 140/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9021 - accuracy: 0.6121 - val_loss: 1.2520 - val_accuracy: 0.4808\n",
      "Epoch 141/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9034 - accuracy: 0.6136 - val_loss: 1.2120 - val_accuracy: 0.4869\n",
      "Epoch 142/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9145 - accuracy: 0.6079 - val_loss: 1.2680 - val_accuracy: 0.4577\n",
      "Epoch 143/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9043 - accuracy: 0.6177 - val_loss: 1.2216 - val_accuracy: 0.4831\n",
      "Epoch 144/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9001 - accuracy: 0.6175 - val_loss: 1.2536 - val_accuracy: 0.4715\n",
      "Epoch 145/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9027 - accuracy: 0.6115 - val_loss: 1.2350 - val_accuracy: 0.4769\n",
      "Epoch 146/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9050 - accuracy: 0.6132 - val_loss: 1.2209 - val_accuracy: 0.4831\n",
      "Epoch 147/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9032 - accuracy: 0.6179 - val_loss: 1.2330 - val_accuracy: 0.4831\n",
      "Epoch 148/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9038 - accuracy: 0.6117 - val_loss: 1.2076 - val_accuracy: 0.5138\n",
      "Epoch 149/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9005 - accuracy: 0.6146 - val_loss: 1.1755 - val_accuracy: 0.5285\n",
      "Epoch 150/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9075 - accuracy: 0.6146 - val_loss: 1.1951 - val_accuracy: 0.4931\n",
      "Epoch 151/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9032 - accuracy: 0.6102 - val_loss: 1.2484 - val_accuracy: 0.4846\n",
      "Epoch 152/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9044 - accuracy: 0.6138 - val_loss: 1.2087 - val_accuracy: 0.4777\n",
      "Epoch 153/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8998 - accuracy: 0.6113 - val_loss: 1.2117 - val_accuracy: 0.4985\n",
      "Epoch 154/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8996 - accuracy: 0.6163 - val_loss: 1.2610 - val_accuracy: 0.4715\n",
      "Epoch 155/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9010 - accuracy: 0.6127 - val_loss: 1.2047 - val_accuracy: 0.4969\n",
      "Epoch 156/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8995 - accuracy: 0.6169 - val_loss: 1.2396 - val_accuracy: 0.4777\n",
      "Epoch 157/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8991 - accuracy: 0.6169 - val_loss: 1.2894 - val_accuracy: 0.4508\n",
      "Epoch 158/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9067 - accuracy: 0.6113 - val_loss: 1.2513 - val_accuracy: 0.4631\n",
      "Epoch 159/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9014 - accuracy: 0.6130 - val_loss: 1.2330 - val_accuracy: 0.4900\n",
      "Epoch 160/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9036 - accuracy: 0.6138 - val_loss: 1.1789 - val_accuracy: 0.5246\n",
      "Epoch 161/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9067 - accuracy: 0.6065 - val_loss: 1.2522 - val_accuracy: 0.4700\n",
      "Epoch 162/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9047 - accuracy: 0.6140 - val_loss: 1.2450 - val_accuracy: 0.4692\n",
      "Epoch 163/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8987 - accuracy: 0.6167 - val_loss: 1.2451 - val_accuracy: 0.4769\n",
      "Epoch 164/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9007 - accuracy: 0.6079 - val_loss: 1.2368 - val_accuracy: 0.4969\n",
      "Epoch 165/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9025 - accuracy: 0.6188 - val_loss: 1.2157 - val_accuracy: 0.4862\n",
      "Epoch 166/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9000 - accuracy: 0.6159 - val_loss: 1.2795 - val_accuracy: 0.4623\n",
      "Epoch 167/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9003 - accuracy: 0.6159 - val_loss: 1.2079 - val_accuracy: 0.4969\n",
      "Epoch 168/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9020 - accuracy: 0.6138 - val_loss: 1.2296 - val_accuracy: 0.4792\n",
      "Epoch 169/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9028 - accuracy: 0.6165 - val_loss: 1.2046 - val_accuracy: 0.5015\n",
      "Epoch 170/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9064 - accuracy: 0.6107 - val_loss: 1.2197 - val_accuracy: 0.5023\n",
      "Epoch 171/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8982 - accuracy: 0.6132 - val_loss: 1.2761 - val_accuracy: 0.4546\n",
      "Epoch 172/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8971 - accuracy: 0.6175 - val_loss: 1.2470 - val_accuracy: 0.4692\n",
      "Epoch 173/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8957 - accuracy: 0.6184 - val_loss: 1.2101 - val_accuracy: 0.5015\n",
      "Epoch 174/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9012 - accuracy: 0.6146 - val_loss: 1.2121 - val_accuracy: 0.4938\n",
      "Epoch 175/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9033 - accuracy: 0.6155 - val_loss: 1.2721 - val_accuracy: 0.4577\n",
      "Epoch 176/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8992 - accuracy: 0.6127 - val_loss: 1.2317 - val_accuracy: 0.4623\n",
      "Epoch 177/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9012 - accuracy: 0.6136 - val_loss: 1.2096 - val_accuracy: 0.4977\n",
      "Epoch 178/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.9000 - accuracy: 0.6125 - val_loss: 1.1944 - val_accuracy: 0.5269\n",
      "Epoch 179/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9060 - accuracy: 0.6184 - val_loss: 1.2224 - val_accuracy: 0.4954\n",
      "Epoch 180/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9007 - accuracy: 0.6107 - val_loss: 1.2508 - val_accuracy: 0.4862\n",
      "Epoch 181/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8976 - accuracy: 0.6129 - val_loss: 1.2159 - val_accuracy: 0.4954\n",
      "Epoch 182/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8980 - accuracy: 0.6134 - val_loss: 1.1986 - val_accuracy: 0.5015\n",
      "Epoch 183/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.9018 - accuracy: 0.6105 - val_loss: 1.1968 - val_accuracy: 0.4969\n",
      "Epoch 184/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8970 - accuracy: 0.6196 - val_loss: 1.2367 - val_accuracy: 0.5115\n",
      "Epoch 185/2000\n",
      "5197/5197 [==============================] - 0s 5us/sample - loss: 0.8978 - accuracy: 0.6175 - val_loss: 1.2282 - val_accuracy: 0.4885\n",
      "Epoch 186/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8963 - accuracy: 0.6155 - val_loss: 1.2465 - val_accuracy: 0.4731\n",
      "Epoch 187/2000\n",
      "5197/5197 [==============================] - 0s 6us/sample - loss: 0.8965 - accuracy: 0.6179 - val_loss: 1.2434 - val_accuracy: 0.4854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a0acb49630>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y_en, validation_split = 0.2, epochs = 2000, batch_size = 500, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "republican-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497/6497 [==============================] - 0s 36us/sample - loss: 0.9623 - accuracy: 0.5950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.962274768938922, 0.59504384]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-antique",
   "metadata": {},
   "source": [
    "### 과적합방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "operating-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "coral-grave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim = 12, activation = 'relu')) # 입력층\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(48, activation = 'relu'))  # 은닉층1\n",
    "model.add(Dropout(0.75)) \n",
    "model.add(Dense(24, activation = 'relu'))  # 은닉층2\n",
    "model.add(Dropout(0.6)) \n",
    "model.add(Dense(7, activation = 'softmax')) # 출력층\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
