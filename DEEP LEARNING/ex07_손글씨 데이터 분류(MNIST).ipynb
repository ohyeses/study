{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 MNIST (다중분류)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 한 개만 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digit = train_images[100]\n",
    "label = train_labels[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(digit, cmap = plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label # 정답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000, 28 * 28)\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape(10000, 28 * 28)\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_one = to_categorical(train_labels)\n",
    "test_labels_one = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_one.shape, test_labels_one.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 신경망 설계(모델 설계)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense(512, activation = 'relu', input_dim=784)) \n",
    "model3.add(Dense(10, activation = 'softmax'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.2446 - accuracy: 0.9295\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0964 - accuracy: 0.9719\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0623 - accuracy: 0.9811\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0430 - accuracy: 0.9877\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0321 - accuracy: 0.9900\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0227 - accuracy: 0.9935\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0188 - accuracy: 0.9944\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.0128 - accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0111 - accuracy: 0.9968\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0110 - accuracy: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20845298f60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_images, train_labels_one, epochs = 10, batch_size = 84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0654 - accuracy: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06543431079589354, 0.9822]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(test_images, test_labels_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0377754e-10, 1.3658218e-10, 4.1484562e-08, 1.1640405e-05,\n",
       "        3.1165597e-15, 9.8774198e-09, 9.3179672e-15, 9.9998808e-01,\n",
       "        1.7157182e-09, 1.9914130e-07]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict([test_images[0:1]]) # 인덱스 0번째"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_images[0:1].reshape(28,28), cmap = plt.cm.binary) # 색깔을 2가지 색깔 흑백\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 CNN 모델링\n",
    "* 사진의 원형을 유지하며 학습가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test data reshape\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 CNN 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model = Sequential()\n",
    "# # 1. feature extractor => 특징 추출부\n",
    "# cnn_model.add(Conv2D(input_shape = (28, 28, 1), # 입력 데이터의 shape 정보\n",
    "#                      filters = 3, #  수\n",
    "#                     kernel_size = (3,3),#돋보기 크기\n",
    "#                      activation=\"relu\"\n",
    "#                     ))\n",
    "\n",
    "# cnn_model.add(MaxPool2D())\n",
    "\n",
    "# cnn_model.add(Conv2D(filters = 3, #  수\n",
    "#                     kernel_size = (3,3),#돋보기 크기\n",
    "#                      activation=\"relu\"\n",
    "#                     ))\n",
    "\n",
    "# cnn_model.add(MaxPool2D())\n",
    "# # 2. classifier => MLP (추출된 특징으로 분류)\n",
    "# cnn_model.add(MaxPool2D(Flatten()) # 데이터를 1차원으로 펴주는 층\n",
    "# model.add(Dense(512,activation=\"relu\"))\n",
    "# model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "cnn_model = Sequential()\n",
    "# 1. feature extraction \n",
    "\n",
    "cnn_model.add(Conv2D(input_shape = (28, 28, 1), # shape of input data\n",
    "                    filters = 3, # number of magnifying glasses(MG)\n",
    "                    # thus, the more you bring MG, the more diverse features will be extracted\n",
    "                    kernel_size = (3, 3), # the size of MG  : 3 * 3 = 9\n",
    "                    activation = \"relu\"\n",
    "                    ))\n",
    "cnn_model.add(MaxPool2D())\n",
    "\n",
    "\n",
    "cnn_model.add(Conv2D(filters = 3, # number of magnifying glasses(MG)\n",
    "                    # thus, the more you bring MG, the more diverse features will be extracted\n",
    "                    kernel_size = (3, 3), # the size of MG  : 3 * 3 = 9\n",
    "                    activation = \"relu\"\n",
    "                    ))\n",
    "cnn_model.add(MaxPool2D())\n",
    "# 2. classifier - MLP(classify data according to extracted feature\n",
    "cnn_model.add(Flatten()) # layer that makes data with 1 dimension (simplication)\n",
    "cnn_model.add(Dense(512, activation = \"relu\"))\n",
    "cnn_model.add(Dense(10, activation = \"softmax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 3)         30        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 3)         84        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               38912     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 44,156\n",
      "Trainable params: 44,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/15\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.3374 - accuracy: 0.8973 - val_loss: 0.1633 - val_accuracy: 0.9474\n",
      "Epoch 2/15\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.1337 - accuracy: 0.9586 - val_loss: 0.1270 - val_accuracy: 0.9592\n",
      "Epoch 3/15\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.0984 - accuracy: 0.9681 - val_loss: 0.1114 - val_accuracy: 0.9652\n",
      "Epoch 4/15\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.0782 - accuracy: 0.9750 - val_loss: 0.0924 - val_accuracy: 0.9729\n",
      "Epoch 5/15\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.0676 - accuracy: 0.9780 - val_loss: 0.0933 - val_accuracy: 0.9708\n",
      "Epoch 6/15\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.0568 - accuracy: 0.9820 - val_loss: 0.0767 - val_accuracy: 0.9767\n",
      "Epoch 7/15\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.0492 - accuracy: 0.9841 - val_loss: 0.0729 - val_accuracy: 0.9777\n",
      "Epoch 8/15\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.0452 - accuracy: 0.9857 - val_loss: 0.0817 - val_accuracy: 0.9748\n",
      "Epoch 9/15\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.0392 - accuracy: 0.9870 - val_loss: 0.0817 - val_accuracy: 0.9764\n",
      "Epoch 10/15\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.0340 - accuracy: 0.9882 - val_loss: 0.0707 - val_accuracy: 0.9804\n",
      "Epoch 11/15\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.0316 - accuracy: 0.9893 - val_loss: 0.0767 - val_accuracy: 0.9796\n",
      "Epoch 12/15\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.0739 - val_accuracy: 0.9796\n",
      "Epoch 13/15\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.0254 - accuracy: 0.9913 - val_loss: 0.0778 - val_accuracy: 0.9789\n",
      "Epoch 14/15\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.0206 - accuracy: 0.9932 - val_loss: 0.0801 - val_accuracy: 0.9803\n",
      "Epoch 15/15\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.0205 - accuracy: 0.9926 - val_loss: 0.0775 - val_accuracy: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2080528cac8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_images, train_labels_one, epochs=15, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0685 - accuracy: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06851088715406077, 0.9806]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(test_images, test_labels_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 데이터의 문제점 \n",
    "* 손글씨에 데이터에 배경이 없다\n",
    "* 손글씨의 크기가 일정하고, 회전도 없다.\n",
    "* MLP와 CNN모델의 성능이 비슷하다\n",
    "* 데이터에 회전을 해서 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000, 28, 28)\n",
    "test_images = test_images.reshape(10000, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle = np.random.randint(0, 360)\n",
    "angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x208096a5630>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOy0lEQVR4nO3df6wV9ZnH8c8jPxW05Up7e4s/QAtN1FRs76JdidXYNeg2otuthTQutqbXrrCh2WazRDet3d0Yd1PstvZHgistbazVhhpIa10o6YrWSrkaQBAVRCiwV1hLLNSuwL08+8cdzFXvfM9lZs4PeN6v5OacM8+ZmSdHP8ycmTnzNXcXgBPfSc1uAEBjEHYgCMIOBEHYgSAIOxDE8EaubKSN8tEa08hVAqG8odd1yA/aYLVSYTezGZK+IWmYpP9097tS7x+tMbrYriyzSgAJa3xVbq3wbryZDZP0bUlXSzpP0mwzO6/o8gDUV5nv7NMkbXX3be5+SNKPJc2spi0AVSsT9gmSdg54vSub9hZm1mVm3WbWfVgHS6wOQBl1Pxrv7ovcvdPdO0doVL1XByBHmbDvlnTmgNdnZNMAtKAyYV8rabKZTTKzkZJmSVpeTVsAqlb41Ju795rZPEn/pf5Tb4vdfVNlnQGoVKnz7O7+iKRHKuoFQB1xuSwQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiFJDNpvZdkkHJPVJ6nX3ziqaAlC9UmHPXOHur1awHAB1xG48EETZsLukFWb2tJl1DfYGM+sys24z6z6sgyVXB6Cosrvx0919t5m9V9JKM3ve3VcPfIO7L5K0SJJOszYvuT4ABZXasrv77uxxr6SHJU2roikA1SscdjMbY2anHn0u6SpJG6tqDEC1yuzGt0t62MyOLudH7v5oJV3huPGn6y9O1k9Z1p1fPNJXcTdIKRx2d98m6cIKewFQR5x6A4Ig7EAQhB0IgrADQRB2IIgqfgjTMFvvviS39v7Hy12cN2bn68m6d3MJwWB+ds9/JOv3fPWi3NpT+yYl5937vYnJ+rglv0nW8VZs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCHNv3M1jTrM2v9iurMuy79/562T99p6PJ+svfvmCZH3ko2uPuafjwbDJ5yTrY7/3h2T9/kkrCq/7oB9O1kfZiGT9tj3pmxlv6Exsy07Qn9eu8VXa7/tssBpbdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0I4rj6PXvK+GFjkvUF7SuT9U9NmJqsn36sDR0nrO9Isv709rOS9SOT0vO/4b25tVNsZHLeWu5sT9ymWtKf3zQvtzb+J+n7Exw5cKBQT62MLTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBHHCnGf/cPenk/U1H/lRsj7qU3uS9d/ro7m10+9r4fuX26A/bX7Tls93JOvPX/GtZP3P/m1+st7+zSdza6Mee19y3qUf+HmyXstHb80/D7/lJ6NLLft4VHPLbmaLzWyvmW0cMK3NzFaa2ZbscVx92wRQ1lB2478vacbbpi2QtMrdJ0talb0G0MJqht3dV0va97bJMyUtyZ4vkXRdtW0BqFrR7+zt7t6TPX9FUnveG82sS1KXJI3WKQVXB6Cs0kfjvf+Olbl3rXT3Re7e6e6dIzSq7OoAFFQ07HvMrEOSsse91bUEoB6Khn25pDnZ8zmSllXTDoB6qfmd3cwekHS5pPFmtkvSVyTdJekhM7tZ0g5JN9SzyaF4z7UvJOszP/DX6QWkTyerd0z6fHWr2nl7/vUBkrTpb+4ptfz3P7g1WU/dnX3juonJeVefkf69+2WjDyXrCzueyi8+n5xV83ZPT9Z3XpXure+19P32m6Fm2N19dk6pPqM9AKgLLpcFgiDsQBCEHQiCsANBEHYgiBNmyOZaDq6YmKz/fY2hhz9+8mu5tZ6+9Cmgq5+cm6xPmr0+WS9j19LzS82/7pIfJOt/ecPNybr9el3hdR+ZPjVZf6krva164cp7C6+7lg8+fGuyPnnemrqtO4UhmwEQdiAKwg4EQdiBIAg7EARhB4Ig7EAQJ8ytpGvZ8dJ7k/Wrz681RO+w3MpZw09Ozvnk9O8k65/RpTXWXdy7Hjo1WV/wr+nz6C/3vlFlO8fkpCfWJetT3rggWV968fjc2ifHvlqkpTetvnZhst71L3+VrPftafz9XtiyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQYX7PXstLCy9J1jfP+nbhZV+/5RPJ+pHr/pSs17wt8Un51wD8bOdvk7P+rvf/kvVZX/2HZL1tcesOV73jn9O30U6Z+8lHkvUvvHtb4WVL0pRlf5tfuzX93yyF37MDIOxAFIQdCIKwA0EQdiAIwg4EQdiBIML8nr2WD35te/oNs4ov+5wav50e91j6PPvyRR9L1s//zHPH3NNR8679fLLetr51z6PXcvaX83sfPuns5LwPrp+RrH/hm+l7FLSimlt2M1tsZnvNbOOAaXeY2W4zW5f9XVPfNgGUNZTd+O9LGuyfua+7+9TsL325EYCmqxl2d18taV8DegFQR2UO0M0zsw3Zbv64vDeZWZeZdZtZ92EdLLE6AGUUDft3JZ0raaqkHkm5d99z90Xu3ununSM0quDqAJRVKOzuvsfd+9z9iKR7JU2rti0AVSsUdjPrGPDyekkb894LoDXUPM9uZg9IulzSeDPbJekrki43s6mSXNJ2SbfUr8Xj38KOp0rN/0+3baiok3eyHf9Tt2W3st6XdyTr76pxn4cHDrQn67NP3ZOsz//YitzaL/Tu5LxF1Qy7u88eZPJ9degFQB1xuSwQBGEHgiDsQBCEHQiCsANBcCvpIbLh+ScuXls+MTnv4xc+WHE3b3X+6s/l1g7vH5mcd8ota6tu54QwfOJZyXrv9t8l6/t/cW6yvvpDDx1zT0d9YsJHcmvcShoAYQeiIOxAEIQdCIKwA0EQdiAIwg4Ewa2kK7DvD2PquvwLv/N3yfqkOxO3e27gdRQnklrn0Wtp+9zryfqKx/P/n7nq5PS8RbFlB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgOM8+RN7bm1s7+aka59kvK7fu9bfek6xf8KHP5tbOnbs7OW/fq78v1BPSenteSdbv3Jo/8PG8l8cn552i3xbqiS07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBefYKvO8bTybr1z52Y7K+/Oc/TNZrDQ982qP55/k5j96axs7YllubovxaGTW37GZ2ppn9ysyeM7NNZjY/m95mZivNbEv2OK4uHQKoxFB243slfcndz5N0iaS5ZnaepAWSVrn7ZEmrstcAWlTNsLt7j7s/kz0/IGmzpAmSZkpakr1tiaTr6tQjgAoc03d2M5so6SJJayS1u3tPVnpF0qBfLM2sS1KXJI3WKYUbBVDOkI/Gm9lYSUslfdHd9w+sef/okIPe2dDdF7l7p7t3jtCoUs0CKG5IYTezEeoP+v3u/tNs8h4z68jqHZL21qdFAFWouRtvZibpPkmb3f3uAaXlkuZIuit7XFaXDk8AdvBwsv7i4UPJ+oNXdCbrbT2JW0kDmaF8Z79U0o2SnjWzddm029Qf8ofM7GZJOyTdUJcOAVSiZtjd/QlJgw7uLunKatsBUC9cLgsEQdiBIAg7EARhB4Ig7EAQ/MS1Afo2b0nW5980N1kf1vNMle0gKLbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE59lbwLD/5jw66o8tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRRM+xmdqaZ/crMnjOzTWY2P5t+h5ntNrN12d819W8XQFFDuXlFr6QvufszZnaqpKfNbGVW+7q7f61+7QGoylDGZ++R1JM9P2BmmyVNqHdjAKp1TN/ZzWyipIskrckmzTOzDWa22MzG5czTZWbdZtZ9WAfLdQugsCGH3czGSloq6Yvuvl/SdyWdK2mq+rf8Cwebz90XuXunu3eO0KjyHQMoZEhhN7MR6g/6/e7+U0ly9z3u3ufuRyTdK2la/doEUNZQjsabpPskbXb3uwdM7xjwtuslbay+PQBVGcrR+Esl3SjpWTNbl027TdJsM5sqySVtl3RLHfoDUJGhHI1/QpINUnqk+nYA1AtX0AFBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Iwd2/cysz+V9KOAZPGS3q1YQ0cm1btrVX7kuitqCp7O9vd3zNYoaFhf8fKzbrdvbNpDSS0am+t2pdEb0U1qjd244EgCDsQRLPDvqjJ609p1d5atS+J3opqSG9N/c4OoHGavWUH0CCEHQiiKWE3sxlm9oKZbTWzBc3oIY+ZbTezZ7NhqLub3MtiM9trZhsHTGszs5VmtiV7HHSMvSb11hLDeCeGGW/qZ9fs4c8b/p3dzIZJelHSX0jaJWmtpNnu/lxDG8lhZtsldbp70y/AMLPLJP1R0g/c/YJs2r9L2ufud2X/UI5z939skd7ukPTHZg/jnY1W1DFwmHFJ10m6SU387BJ93aAGfG7N2LJPk7TV3be5+yFJP5Y0swl9tDx3Xy1p39smz5S0JHu+RP3/szRcTm8twd173P2Z7PkBSUeHGW/qZ5foqyGaEfYJknYOeL1LrTXeu0taYWZPm1lXs5sZRLu792TPX5HU3sxmBlFzGO9Getsw4y3z2RUZ/rwsDtC903R3/7CkqyXNzXZXW5L3fwdrpXOnQxrGu1EGGWb8Tc387IoOf15WM8K+W9KZA16fkU1rCe6+O3vcK+lhtd5Q1HuOjqCbPe5tcj9vaqVhvAcbZlwt8Nk1c/jzZoR9raTJZjbJzEZKmiVpeRP6eAczG5MdOJGZjZF0lVpvKOrlkuZkz+dIWtbEXt6iVYbxzhtmXE3+7Jo+/Lm7N/xP0jXqPyL/kqTbm9FDTl/nSFqf/W1qdm+SHlD/bt1h9R/buFnS6ZJWSdoi6ZeS2lqotx9KelbSBvUHq6NJvU1X/y76Bknrsr9rmv3ZJfpqyOfG5bJAEBygA4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEg/h+0oXJWQI/snQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rotate_img = Image.fromarray(train_images[0]).rotate(angle)\n",
    "plt.imshow(rotate_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_rotate_list = [] \n",
    "for i in range(60000):\n",
    "    angle = np.random.randint(0,360) # 회전시킬 각도 랜덤하게 뽑기\n",
    "    rotate_img = Image.fromarray(train_images[0]).rotate(angle) # 각도 만큼 사진 회전\n",
    "    train_images_rotate_list.append(np.array(rotate_img)) # numpy 타입으로 변경 후 추가\n",
    "\n",
    "train_image_rotate = np.array(train_images_rotate_list) # 전체 list -> numpy 변경\n",
    "train_image_rotate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_rotate_list = [] \n",
    "for i in range(10000):\n",
    "    angle = np.random.randint(0,360) # 회전시킬 각도 랜덤하게 뽑기\n",
    "    rotate_img = Image.fromarray(test_images[0]).rotate(angle) # 각도 만큼 사진 회전\n",
    "    test_images_rotate_list.append(np.array(rotate_img)) # numpy 타입으로 변경 후 추가\n",
    "\n",
    "test_image_rotate = np.array(test_images_rotate_list) # 전체 list -> numpy 변경\n",
    "test_image_rotate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.6 회전된 데이터로 MLP와 CNN 비교(숙제)\n",
    "MLP : 데이터 shape -> 784로 펴주기\n",
    "CNN : 데이터 shape -> (sample, width, height, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Flatten()) \n",
    "mlp_model.add(Dense(512, activation=\"relu\"))\n",
    "mlp_model.add(Dense(1024, activation=\"relu\"))\n",
    "mlp_model.add(Dense(512, activation=\"relu\"))\n",
    "mlp_model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "mlp_model.compile(loss=\"categorical_crossentropy\", \n",
    "                 optimizer=\"Adam\",\n",
    "                 metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/15\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 3.1340 - accuracy: 0.1126 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 2/15\n",
      "42000/42000 [==============================] - 3s 67us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 3/15\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 4/15\n",
      "42000/42000 [==============================] - 3s 66us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 5/15\n",
      "42000/42000 [==============================] - 3s 66us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 6/15\n",
      "42000/42000 [==============================] - 3s 67us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 7/15\n",
      "42000/42000 [==============================] - 3s 66us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 8/15\n",
      "42000/42000 [==============================] - 3s 66us/sample - loss: 2.3012 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 9/15\n",
      "42000/42000 [==============================] - 3s 67us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3021 - val_accuracy: 0.1079\n",
      "Epoch 10/15\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 11/15\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 12/15\n",
      "42000/42000 [==============================] - 3s 66us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 13/15\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 14/15\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 15/15\n",
      "42000/42000 [==============================] - 3s 67us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20842bd8780>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.fit(train_image_rotate, train_labels_one, epochs=15, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_rotate = train_image_rotate.reshape(60000,28,28,1)\n",
    "test_image_rotate = test_image_rotate.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(input_shape=(28,28,1),\n",
    "                     filters=16, # 특징을 도출해주는 돋보기 개수\n",
    "                     kernel_size=(3,3), # 돋보기 크기\n",
    "                     activation = 'relu'\n",
    "                    ))\n",
    "cnn_model.add(MaxPool2D()) # 불필요한 정보 삭제\n",
    "\n",
    "cnn_model.add(Conv2D(filters=32, # 특징을 도출해주는 돋보기 개수\n",
    "                     kernel_size=(3,3),# 돋보기 크기\n",
    "                    activation = 'relu'))\n",
    "cnn_model.add(MaxPool2D()) # 불필요한 정보 삭제\n",
    "\n",
    "# 분류\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation=\"relu\"))\n",
    "cnn_model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss=\"categorical_crossentropy\", \n",
    "                 optimizer=\"Adam\",\n",
    "                 metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/35\n",
      "42000/42000 [==============================] - 4s 93us/sample - loss: 2.7554 - accuracy: 0.1093 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 2/35\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 3/35\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 4/35\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 5/35\n",
      "42000/42000 [==============================] - 3s 83us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 6/35\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 7/35\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3021 - val_accuracy: 0.1079\n",
      "Epoch 8/35\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 9/35\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 10/35\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 11/35\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3022 - val_accuracy: 0.1079\n",
      "Epoch 12/35\n",
      "42000/42000 [==============================] - 4s 86us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 13/35\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 14/35\n",
      "42000/42000 [==============================] - 4s 85us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 15/35\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 16/35\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 17/35\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 18/35\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3017 - val_accuracy: 0.1079\n",
      "Epoch 19/35\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.3012 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 20/35\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 21/35\n",
      "42000/42000 [==============================] - 3s 83us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 22/35\n",
      "42000/42000 [==============================] - 4s 88us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3021 - val_accuracy: 0.1079\n",
      "Epoch 23/35\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3017 - val_accuracy: 0.1079\n",
      "Epoch 24/35\n",
      "42000/42000 [==============================] - 4s 87us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3017 - val_accuracy: 0.1079\n",
      "Epoch 25/35\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.3012 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 26/35\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 27/35\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3018 - val_accuracy: 0.1079\n",
      "Epoch 28/35\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 29/35\n",
      "42000/42000 [==============================] - 3s 83us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 30/35\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3021 - val_accuracy: 0.1079\n",
      "Epoch 31/35\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n",
      "Epoch 32/35\n",
      "42000/42000 [==============================] - 4s 89us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3017 - val_accuracy: 0.1079\n",
      "Epoch 33/35\n",
      "42000/42000 [==============================] - 4s 85us/sample - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3017 - val_accuracy: 0.1079\n",
      "Epoch 34/35\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 2.3012 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1079\n",
      "Epoch 35/35\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 2.3012 - accuracy: 0.1143 - val_loss: 2.3019 - val_accuracy: 0.1079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20805208da0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_image_rotate, train_labels_one, epochs=35, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
